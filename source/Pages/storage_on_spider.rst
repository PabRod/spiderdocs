.. warning:: Please note that Spider is a fresh platform - still in Beta phase - and the documentation here is heavily under construction. If you need any help in these pages, please contact :ref:`our helpdesk <helpdesk>`.

.. _storage-on-spider:

*****************
Storage on Spider
*****************

.. Tip:: Spider is meant for processing of big data, thus is supports several storage backends. In this page you will learn:

     * which internal and external storage system we support
     * bbest practices to manage your data


.. _internal-storage:

================
Internal storage
================

.. _transfers-within-spider:

Transfers transfers within Spider
=================================

To transfer data between directories located within
:abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)` we advise
you to use the unix commands ``cp`` and ``rsync``. Other options may be
available, but these are currently not supported by us.

Help on these commands can be found by (i) typing ``man cp`` or ``man rsync``
on the command line after logging into the system, or (ii) by contacting
:ref:`our helpdesk <helpdesk>`.


.. _filesystems:

Spider filesystems
==================

.. _home-fs:

Using Home
----------

:abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)` provides to each user with a globally mounted
home directory that is listed as ``/home/[USERNAME]``. This directory is
accessible from all nodes.
This is also the directory that you as a user will find yourself in :ref:`upon first
login <getting-around>` into this system. The data stored in the home folder will
remain available for the duration of your project.

.. _scratch-fs:

Using scratch
-------------

Each of :abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)` worker nodes has a large scratch area on local SSD.
These scratch directories enable particularly efficient data I/O for large data
processing pipelines that can be split up into many parallel independent jobs.

Please note that you should only use the scratch space to temporarily store and
process data for the duration of the submitted job. The scratch space is cleaned
regularly in an automatic fashion and hence can not used for long term storage.


.. _project-space-fs:

Using project spaces
--------------------

Coming soon ..

.. _scientific-catalog-fs:

Using scientific catalogs
-------------------------

Coming soon ..



.. _external-storage:

================
External storage
================

.. _transfers-to-and-from-spider:

Transfers to/from Spider
========================

If you are logged in as a user on Spider then we support ``scp``, ``rsync``,
``curl`` or ``wget`` to transfer data between Spider and your own Unix-based system.
Other options may be available, but these are currently not supported by us.

* Example of transferring data from Spider to your own Unix-based system:

.. code-block:: bash

        scp /home/[USERNAME]/transferdata.tar.gz [own-system-user]@own_system.nl:/home/[own-system-user]/
        rsync -a -W /home/[USERNAME]/transferdata.tar.gz [own-system-user]@own_system.nl:/home/[own-system-user]/

* Example of retrieving data from own Unix-based system on Spider:

.. code-block:: bash

        scp [own-system-user]@own_system.nl:/home/[own-system-user]/transferdata.tar.gz /home/[USERNAME]/
        rsync -a -W [own-system-user]@own_system.nl:/home/[own-system-user]/transferdata.tar.gz /home/[USERNAME]/


.. _surfsara-systems:

SURFsara systems
================


.. _using-dcache:

Using dCache
------------

SURFsara hosts a large storage system which consists of magnetic tape storage
and hard disk storage. It uses `dCache system`_ that can store and retrieve
huge amounts of data, distributed among a large number of heterogenous server nodes,
under a single virtual filesystem tree. You may use the storage if your data does not
fit within the storage allocation on Spider project space, or if you wish to use
Tape storage.

There are several storage clients that can interact with dCache and we provide
here some examples with the clients we support on :abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)`. To use these clients you need to have an
X509 Grid certificate and be a part of a Virtual Organisation (VO). Please
refer to our Grid documentation page for instructions on `how to get a certificate`_
and `join a (VO)`_.

You may also transfer data to/from your project spaces.

To be able to interact with dCache using a storage client, you need to
create a proxy with ``voms-proxy-init --voms [YOUR_VO]``. A proxy is a short-lived
certificate/private key combination which is
used to perform actions on your behalf without using passwords. On :abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)` your proxy is generated by default
in your ``$HOME/.proxy`` location such that it is
accessible from anywhere on Spider. You can check this with ``echo X509_USER_PROXY``.

There are many clients to interact with dCache. On :abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)` we support ``globus-url-copy`` and ``gfal``.

Examples of using dCache within :abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)` can be found :ref:`here <dcache-examples>`.


.. _using-swift:

Using SWIFT
------------

Coming soon ..

.. _using-archive:

Using Central archive
---------------------

For long-term preservation of precious data SURFsara offers the `Data Archive`_.
Data ingested into the Data Archive is kept in two different tape libraries
at two different locations in The Netherlands. The Data Archive is connected
to all compute infrastructures, including :abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)`.

Access on Data Archive is *not* provided by default to the :abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)` projects. To request for Data Archive access, please contact our
:ref:`our helpdesk <helpdesk>`.

If you already have access on Data Archive, then you can use it directly from :abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)` by using ``scp`` and ``rsync`` to transfer data between Spider and Data Archive:


* Transfer data from Spider to Data Archive:

.. code-block:: bash

        scp /home/[USERNAME]transferdata.tar.gz [ARCHIVE_USERNAME]@archive.surfsara.nl:/home/[ARCHIVE_USERNAME]/
        rsync -a -W /home/[USERNAME]/transferdata.tar.gz [ARCHIVE_USERNAME]@archive.surfsara.nl:/home/[ARCHIVE_USERNAME]/

* Retrieve data from Data Archive on Spider:

.. code-block:: bash

        scp [ARCHIVE_USERNAME]@archive.surfsara.nl:/home/[ARCHIVE_USERNAME]/transferdata.tar.gz /home/[USERNAME]/
        rsync -a -W [ARCHIVE_USERNAME]@archive.surfsara.nl:/home/[ARCHIVE_USERNAME]/transferdata.tar.gz /home/[USERNAME]/

In case that the file to be retrieved from Data Archive to Spider is not
directly available on disk then the scp/rsync command will hang until the file is
moved from tape to disk. Data Archive users can query the state of their files by
logging into the Data Archive user interface and performing a ``dmls -l`` on the files
of interest. Here the state of the file is either on disk (REG) or on tape (OFL).
The Data Archive user interface is accessible via ``ssh`` from anywhere for users that
have a login account and an example is given below:

.. code-block:: console

        ssh [ARCHIVE_USERNAME]@archive.surfsara.nl
	      touch test.txt
	      dmls  -l test.txt
	      -rw-r--r--  1 homer    homer    0 2019-04-25 15:24 (REG) test.txt

Best practices for the usage of Data Archive are described on the `Data Archive`_ page.



.. _quota-policy:

============
Quota policy
============

Each :abbr:`Spider (Symbiotic Platform(s) for Interoperable Data
Extraction and Redistribution)` is granted specific compute and storage
resources in the context of a project. For these resources there is currently
**no hard quotas**. However, we monitor both the core-hour consumption
and storage usage to prevent that users exceed their granted allocation.

.. _backup-policy:

=============
Backup policy
=============

The data stored on CephFS (home and project spaces) is disk only,
replicated three times for redundancy. For disk-only data there is **no backup**.
If you cannot afford to lose this data, we advise you to copy it elsewhere as well.


.. seealso:: Still need help? Contact :ref:`our helpdesk <helpdesk>`

.. Links:

.. _`Data Archive`: https://userinfo.surfsara.nl/systems/data-archive
.. _`Sylabs documentation`:  https://www.sylabs.io/docs/
.. _`dCache system`: https://www.dcache.org/
.. _`how to get a certificate`: http://doc.grid.surfsara.nl/en/latest/Pages/Basics/prerequisites.html#get-a-grid-certificate
.. _`join a (VO)`: http://doc.grid.surfsara.nl/en/latest/Pages/Basics/prerequisites.html#join-a-virtual-organisation


 .. _`stage`: http://doc.grid.surfsara.nl/en/latest/Pages/Advanced/grid_storage.html#staging-groups-of-files
